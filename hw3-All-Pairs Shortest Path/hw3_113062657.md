---
title: PP HW3 Report

---

# <center>PP HW3 Report</center> <div style="text-align: right; font-size: 16px;">113062657 黃盛揚</div>
## 1.	**Implementation**


* ### Which algorithm do you choose in hw3-1?
我是使用sequential的Floyd-Warshall algorithm，由於他是三層的nested loop，在平行上我是選沒有data dependency的那層，也就是決定處理第幾個row的運算利用omp來進行平行。

---
* ### How do you divide your data in hw3-2, hw3-3?
在HW3-2中，由於每個block的thread數量有1024個，並且為了要實作Blocked Floyd-Warshall algorithm的平行，所以將data切成一個一個block(`size = block factor * block factor`)。而在hw3-3中，也是如此，只是有發現到phase3的時間佔比是最大的，所以在phase3的時候兩個GPU一個做matrix的上半部，一個做matrix的下半部。

---
* ### What’s your configuration in hw3-2, hw3-3? And why? (e.g. blocking factor, #blocks, #threads)
    * **#Threads**：`1024`, 由於在這次的環境中，一個block最多可以有1024個threads。
    
    * **Blocking Factor**：`64`, 首先我發現shared memory最多可以存大約12288個integer，而在phase3中共需要三個shared memory array，分別是要更新的block跟pivot row跟pivot column。而此時一個array可以放進64 * 64個integer，而在實驗後發現，一次load 64 * 64的shared memory跟分四次load 64* 64的shared memory來講比較快，所以最後blocking factor = 64，也就是一個thread會sequential的處理4個elements的運算。
    * **#Blocks**： `matrix_size / blocking factor`, 這邊的matrix_size是對blocking factor padding完的結果，以減少在kernel fuction中出現不必要的warp divergence。

---
* ### How do you implement the communication in hw3-3?
我使用`cudaMemcpy`，若兩張GPU需要data交換，則會先copy到host，在copy到另外一張GPU，是因為在實驗後發現這樣的速度比起device to decive的`cudaMemcpy`還要快，也比`cudaMemcpyPeer`還要快。

---
* ### Briefly describe your implementations in diagrams, figures or sentences.
* **Single-GPU** : 
    * 使用 Blocked Floyd-Warshall 演算法，將不同的 phase 各自實作為獨立的 kernel function：phase1(), phase2(), phase3()。
    * 每個 block 根據自身的 blockIdx 和 threadIdx 決定需要處理的資料：
    * Phase 1, 2：將 pivot block、pivot row、pivot column 載入 shared memory。
    * Phase 3：更新的 block data 無data dependency，直接載入至 register 進行運算，運算後再寫回 global memory。
    
* **Mutiple-GPU** : 
    * Phase 1, 2：兩張 GPU 同時執行相同運算，確保所有 block 能取得 pivot block。
    * Phase 3：
        * GPU 0 處理矩陣的上半部，GPU 1 處理下半部。
        * 每輪運算前檢查該輪的 pivot block 是否最新：
        * 若 pivot block 在上半部，由 GPU 0 傳給 GPU 1。
        * 若 pivot block 在下半部，由 GPU 1 傳給 GPU 0。
        * 傳輸資料量為 blocking factor * matrix_size，因 CUDA 的 cudaMemcpy() 僅支援一維陣列傳輸。


---
## 2.	**Profiling Results**
我使用p11k1為測試data。

| Phase   | Achieved Occupancy | SM Efficiency (%) | Shared Load Throughput (GB/s) | Shared Store Throughput (GB/s) | Global Load Throughput (GB/s) | Global Store Throughput (GB/s) |
|---------|--------------------|-------------------|-------------------------------|--------------------------------|-------------------------------|--------------------------------|
| phase1  | 0.498249           | 4.48              | 123.02                        | 41.431                         | 0.65270                       | 0.65270                       |
| phase2  | 0.973900           | 95.10             | 2551.1                        | 1039.3                         | 31.495                        | 31.495                        |
| phase3  | 0.922892           | 99.89             | 3445.3                        | 143.56                         | 215.33                        | 71.778                        |

## 3.	**Experiment & Analysis**
### a. System Spec

* Apollo GPU
* 使用p11k1做實驗

---

### b. Blocking Factor 
<div style="display: flex; justify-content: center; gap: 10px;">
  <img src="https://hackmd.io/_uploads/BJlFcdNrJg.png" width="350">
  <img src="https://hackmd.io/_uploads/H187hOVB1x.png" width="350">
</div>

當blocking factor上升的同時，可以讓更多threads同時處理運算量，一個block最多可以有1024個threads，也就是當blocking size = 32的時候，會讓每一個threads都可以同時處理data，但當blocking factor上升到64，可以減少Round以及global memory到shared memory的次數，使GOPS更高。而shared memory的部分，可以發現其bandwith是足夠可以將整塊shared memory load滿，所以在blocking factor為64的情況下，phase3的SM Efficiency是99.9%，使其可以完整的使用bandwith，也讓throughput變高。




---

### c. Optimization 
![output (2)](https://hackmd.io/_uploads/ryYvet4rkx.png)

我從加入shared memory的部分開始，並且將phase3要更新的block data改成使用register的方式存取，就可以從本來的61秒降到4.27秒，但只能通過大概到p21k1的測資，所以就調大了blocking factor，從32到64，讓每個thread做4個elements的運算，讓速度有明顯的變高，可以通過p30k1，最後用padding的方式去消除bank conflict，並用`cudaHostRegister`將host memory bind到pinned memory上，加速memory copy的時間，來到最終的1.87秒。

---
### d. Weak scalability

| GPU 數量 | 測資        | 資料大小 (矩陣維度) | 資料比例 | 執行時間 (秒) |
|----------|-------------|----------------------|----------|---------------|
| 1        | c19.1       | 2112 x 2112          | 1.0x     | 0.30873       |
| 2        | c18.1       | 3008 x 3008          | 2.03x    | 0.56074       |

這個表格比較了single GPU 和mutiple GPU 在不同測資下的執行時間與資料比例。用兩張GPU 處理約 2.03 倍的資料時，執行時間為 0.56074 秒，增加約 1.8 倍的運算時間，以結果來講兩兩張GPU的效能提升受到一定的communication cost的影響，但仍然有得到一些speed up。

---
### e. Time Distribution 
<div style="display: flex; justify-content: center;">
  <img src="https://hackmd.io/_uploads/B14GCnrrJx.png" width="400">
</div>

可以發現I/O以及memory copy都佔了不少的時間，而在memmory copy中尤其是device to host的`cudaMemcpy()`，比起我host to device用`cudaMemcpyAsync`時間來的久很多。而computing的時間中phase 3佔據了最大一部分，也充分展現phase 3就是這次運算時間的最大瓶頸，也是我在muti-GPU中特別加速得地方。

---

## 4.	**Experiment on AMD GPU**

首先我觀察到一個很微妙的狀況，假若我今天拿blocking factor = 64的版本，也就是shared memory使用率比較高的版本，在AMD上反而比較慢，但使用原本blocking factor = 32卻可以在AMD版本中獲得更好的結果。我想是在AMD GPU中大量使用 shared memory，可能會和global memory的操作競爭資源，導致性能下降，因為Compute Units (CUs) 的  Local Memory 和global memory。

---

## 5. Experience & conclusion

這次作業讓我學習到如何透過多執行緒 (multi-threading)、單 GPU 和多 GPU 實現 All-Pairs Shortest Path 問題，體驗了 CUDA 加速的效能優勢與資料分區的重要性。透過 Blocked Floyd-Warshall Algorithm 和效能分析工具，我了解了平行化優化的挑戰，例如 Memory Access Efficiency。這次經驗加深了我對平行程式設計的理解，對未來相關研究非常有幫助。
