---
title: PP HW1 Report

---

# <center>PP HW1 Report</center> <div style="text-align: right; font-size: 16px;">113062657 黃盛揚</div>
## 1.	**Implementation**
* ### handle an arbitrary number of input items and processes
這次的作業在handle每個process要處理的量，用平均分配的方式算出local_data_n跟remain_data_n，並把remain_data_n再平均分配給前面的process。若出現process數大於data size的話，用MPI_Comm_split()切出實際上需要用到的process，若process不在新的MPI_Comm則關掉該process。
* ### sorting method
在sorting的部分，每個process先用*boost::sort::spreadsort::float_sort*來sort好各自的data，接著進行odd-even merge，以process為單位(odd phase就是跟偶數rank的process跟奇數rank的process做merge)，並用MPI_Sendrecv()將相鄰的兩個processes的data做傳遞，故每個process會有front array以及back array，每個process只需要merge好自己的array就好。舉個例子，若rank0跟rank1做merge，則rank0只需要由小開始merge，等到merge完front array size的data即可，rank1則是由大開始，並merge完back array size即可。
* ### Other efforts
在跑實驗的時候，有發現到communicate(MPI_Sendrecv)花的時間很長，所以要merge前我會先傳前面array的最後一個跟後面array的第一個，若前者小於後者，則表示merge不會改變兩個process內的data，故就不做merge也就不用MPI_Sendrecv整個array，時間會變得更快。
## 2.	**Experiment & Analysis**
### i. Methodology
我是使用Nsight Systems Profiling以及NVTX來計算CPU time，communication time 跟 IO time。利用set range來找出各別的時間，並用timeline來分析MPI各個function的時間。
### ii. Plots: Speedup Factor & Profile
#### a. Experimental Method
在testcase上我是選case40來做分析，畢竟data_size必較大，在CPU time的optimization也會比較容易可以觀察到。然後我分別使用了1, 4, 16, 32個processes來做比較，並且避免有context switch產生，想讓每個process都可以獨立使用一個core，故用了1, 1, 4, 8個node。
#### b. Analysis of Results
<img src="https://hackmd.io/_uploads/B1wviICeye.jpg" alt="speedup" style="display: block; margin: 0 auto;">

<p style="text-align: center;"><strong>Figure 1 : speed up</strong> </p>

<img src="https://hackmd.io/_uploads/ry0m6IAlJl.jpg" alt="usage" style="display: block; margin: 0 auto;">

<p style="text-align: center;"><strong>Figure 2 : Time profile</strong> </p>

在figure 1上可以發現，隨著processes數量變多，process數為16跟32時上升的幅度很小。這可以從figure 2可以發現，當隨著processes數量變多CPU time會有很明顯的減少，但是communicate會有一定程度的增加，但比起communicate，I/O的加速幅度並沒有想像中的大，尤其是MPI_File_write_at()。雖然MPI_File_write_at()可以支援mutiple write，但在結果上，仍然需要等其他的process做完才會接著做，所以這裡推測是因為file system可能不support mutiple write。而I/O又佔了一大部分的時間(幾乎時間都花在MPI_File_write_at上)，所以這次的作業的Bottleneck基本是在I/O上。
| processes | CPU    | communicate | I/O    |
| --------- | ------ | ----------- | ------ |
| 1         | 21.243 | 0           | 11.522 |
| 4         | 5.506  | 1.105       | 11.634 |
| 16        | 1.981  | 2.883       | 9.569  |
| 32        | 0.698  | 2.772       | 8.94   |

但在這次的作業中，我有針對communicate的Bottleneck做一些處理，也就是Other efforts提到的方法，在processor為32個的情況下最後一次的odd-even phase的MPI_Sendrecv的差異如下:
| phase | Original Time (ns) | Optimized Time (ns) |
|----------|---------------------|---------------------|
| Even     | 201,570,780        | 28,425             |
| Odd      | 95,842,944         | 3,210              |

可以發現有很明顯的差距，且對任一case最多merge $\lceil \frac{\text{Process}}{2} \rceil$次，所以這樣可以對於MPI_Sendrecv的加速有至少1.2倍。我們也可以透過Nsight Systems Profiling發現MPI_Allreduce也滿花時間的平均一次0.214秒，似乎也有優化的空間。
### iii. Discussion
#### Compare I/O, CPU, Network performance. Which is/are the bottleneck(s)? Why? How could it be improved?
#### Compare scalability. Does your program scale well? Why or why not? How can you achieve better scalability?
我認為我並沒有scale得很好，因為I/O跟Network仍是bottlenecks，兩個都很難因為process變多而有顯著的speed up，就算CPU time真的趨近於0，I/O跟communicate還是需要很多時間。也許我可以嘗試減少All_redure跟Sendrecv，或是將先做完的process先做write的動作。
## 3. Experiences / Conclusion
這次的作業學會了mpi的使用方式，也在優化的過程中，發現了很多以前習慣的coding style會很容易造成program跑得很慢，也學會如何透過profiling來檢視program的timeline之類的，挺好的。

