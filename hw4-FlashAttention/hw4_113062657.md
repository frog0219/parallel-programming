---
title: PP HW4 Report

---

# <center>PP HW4 Report</center> <div style="text-align: right; font-size: 16px;">113062657 黃盛揚</div>
## 1.	**Implementation**

### a. Describe how you implemented the FlashAttention forward pass using CUDA. Mention the algorithm's key steps, such as matrix blocking, SRAM usage, and how intermediate results like scaling factors (ℓ and 𝑚) were calculated.
* **Matrix Blocking** : 將 𝑄 Q、 𝐾 K、 𝑉 V 矩陣分割為較小的區塊，這樣可以在shared memory 中處理。 每次只需加載區塊到shared memory中，減少對global memory的頻繁訪問。 

* **SRAM Usage** : 使用共享記憶體儲存 𝑄、 𝐾、 𝑉 的區塊及Intermediate Results，如scaling factors ℓ 和 𝑚。 藉由分塊處理減少global memory bandwidth的需求，提升資料存取效率。
 
* **Intermediate Results** : 在 CUDA kernel中dynamically計算scaling factors ℓ（累積的行總和）和 𝑚（每行的最大值），確保 softmax 計算的數值的正確性。

---

### b.Explain how matrices Q, K, and V are divided into blocks and processed in parallel.

* **Block Division** : 
    * Q 的每個block包含 32 行和 𝑑 列。 
    * 𝐾 的每個block包含 𝑑 行和 32 列。 
    * 𝑉 的每個block包含 32 行和 𝑑 列。
* **Parallel Processing** :
    *  在每個 thread block 中， 32 × 32 = 1024 個 threads 協作完成對應區塊的計算任務。 
    *  每個 thread 計算 $𝑆 = 𝑄 ⋅ 𝐾 ^ ⊤$ 矩陣中的特定元素來完成 softmax。

---

### c.Describe how you chose the block sizes B_r and B_c and why.
由於threads的數量上限為1024且32 是 CUDA warp 的大小所以B_r跟B_c都為32。

---

### d.Specify the configurations for CUDA kernel launches, such as the number of threads per block, shared memory allocation, and grid dimensions.
* 每個 thread block 的 thread 數: 每個block啟動 32 × 32 的 thread。
* shared memory allocation: 使用shared memory儲存 𝑄 、 𝐾 、 𝑉  的區塊，以及暫存用的 ℓ 和 𝑚，而由於 d = {32 , 64}，故  𝑂 、 𝑄 、 𝐾 、 𝑉 直接開BLOCK_SIZE * 64的大小就可以不用extern memory完成宣告
* grid dimensions: Grid = ( N / BLOCK_SIZE )的維度會覆蓋 𝑄 、 𝐾  的所有區塊，確保整個矩陣能並行運算。

---

### e.Justify your choices and how they relate to the blocking factors and the SRAM size.
* 32 × 32 的block確保shared memory的使用量符合 GPU 硬體限制。
* Blocking降低了記憶體latency，並最大化記憶體bandwidth的使用效率。

---


## 2. **Profiling Results**
為跑t20的結果。

| Metric                            | Minimum       | Maximum       | Average       |
|---------------------------------------|---------------|---------------|---------------|
| Achieved Occupancy                    | 0.499931      | 0.499944      | 0.499936      |
| Multiprocessor Activity               | 80.43%        | 81.68%        | 81.24%        |
| Shared Memory Load Throughput         | 2074.1GB/s    | 2142.6GB/s    | 2100.8GB/s    |
| Shared Memory Store Throughput        | 97.428GB/s    | 100.65GB/s    | 98.685GB/s    |
| Global Load Throughput                | 221.70GB/s    | 229.03GB/s    | 224.56GB/s    |
| Global Store Throughput               | 221.21MB/s    | 228.52MB/s    | 224.07MB/s    |


---

## 3. **Experiment & Analysis**
### a. System Spec
* Apollo GPU
* 使用t20做實驗

---

### b. Optimization

![output (3)](https://hackmd.io/_uploads/rybCIRrBJg.png)

一開始助教給得code`seq-flashattention`的runtime為219秒，然後用cuda平行，並統一使用一個kernel function，其餘的運算改用__device__ function，可以將所有要使用的變數移到shared memory中，並且H2D跟D2H的搬移在同一個batch下只需兩次，減少memory copy跟大幅增加shared memory的使用，時間來到1.3秒。
在跑了`nvprof --metrics shared_store_transactions_per_request`的情況下可以發現load 的時候 average bank conflict Per Request為8.5次，所以用padding的方式解決bank conflict，時間降到0.62秒。

---

### c. Other
<div style="display: flex; justify-content: center;">
  <img src="https://hackmd.io/_uploads/BJOEkyLHke.png" width="400">
</div>

有做各項的time distribution，可以發現由於data size很大`memcpy()`佔了很大的比例，也間接表現出FlashAttention平行的結果還不錯。

---

## 4. **Experience & conclusion**

本次的bank conflict比起hw3來說多了很多，在優化上進步的也明顯。然後本次的sequential code對我們平行的幫助非常大，比起hw3在實作上容易很多。
